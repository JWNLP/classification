{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[yelp]ep 4, batch64, ml256,  lr2e-5, dropout01, acc92 .ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JWNLP/classification/blob/main/%5Byelp_p%5D%20acc%20%3D%2094%2C%20max-length%20%3D%20256%2C%20learning-rate%20%3D%205e-5%2C%20drop-out%20%3D%200.1%2C%20epochs%20%3D%204%2C%20batch-size%20%3D32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODT5aounmWtT"
      },
      "source": [
        "yelp_review_polarity \n",
        "nrows = 3000\n",
        "\n",
        "acc = 94, max-length = 256, learning-rate = 5e-5, drop-out = 0.1, epochs = 4, batch-size =32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIwXBwf2l9Hh"
      },
      "source": [
        "## yelp_review_pilaruty_csv\n",
        "\n",
        "> \n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/yelp_review_polarity_csv/train.csv\", delimiter=',', header=None, names=['label', 'sentence'])\n",
        "\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/yelp_review_polarity_csv/test.csv\", delimiter=',', header=None, names=['label', 'sentence'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ1AiW4am5G0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feac8272-fad2-4166-a01e-915e3070760e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbmwCmOsX36p",
        "outputId": "851d7a74-27b7-4fed-e07a-52eb2f95a646"
      },
      "source": [
        "pip install transformers\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/9e/5b80becd952d5f7250eaf8fc64b957077b12ccfe73e9c03d37146ab29712/transformers-4.6.0-py3-none-any.whl (2.3MB)\n",
            "\r\u001b[K     |▏                               | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 29.6MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 23.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 24.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 51kB 23.1MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 20.6MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 21.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 81kB 23.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 92kB 22.3MB/s eta 0:00:01\r\u001b[K     |█▍                              | 102kB 21.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 112kB 21.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 122kB 21.9MB/s eta 0:00:01\r\u001b[K     |█▉                              | 133kB 21.9MB/s eta 0:00:01\r\u001b[K     |██                              | 143kB 21.9MB/s eta 0:00:01\r\u001b[K     |██                              | 153kB 21.9MB/s eta 0:00:01\r\u001b[K     |██▎                             | 163kB 21.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 174kB 21.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 184kB 21.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 194kB 21.9MB/s eta 0:00:01\r\u001b[K     |██▉                             | 204kB 21.9MB/s eta 0:00:01\r\u001b[K     |███                             | 215kB 21.9MB/s eta 0:00:01\r\u001b[K     |███                             | 225kB 21.9MB/s eta 0:00:01\r\u001b[K     |███▎                            | 235kB 21.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 245kB 21.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 256kB 21.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 266kB 21.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 276kB 21.9MB/s eta 0:00:01\r\u001b[K     |████                            | 286kB 21.9MB/s eta 0:00:01\r\u001b[K     |████                            | 296kB 21.9MB/s eta 0:00:01\r\u001b[K     |████▏                           | 307kB 21.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 317kB 21.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 327kB 21.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 337kB 21.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 348kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 358kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 368kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 378kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 389kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 399kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 409kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 419kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 430kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 440kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 450kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 460kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 471kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 481kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 491kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 501kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 512kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 522kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 532kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 542kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 552kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 563kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 573kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 583kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 593kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 604kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 614kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 624kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 634kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 645kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 655kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 665kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 675kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 686kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 696kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 706kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 716kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 727kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 737kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 747kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 757kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 768kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 778kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 788kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 798kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 808kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 819kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 829kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 839kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 849kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 860kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 870kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 880kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 890kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 901kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 911kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 921kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 931kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 942kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 952kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 962kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 972kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 983kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 993kB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.4MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.4MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.4MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.4MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.4MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.4MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.4MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.4MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.4MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.4MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.5MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.5MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.5MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.5MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.5MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.5MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.5MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.5MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.5MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.5MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.6MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.6MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.6MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.6MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.6MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.6MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.6MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.6MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.6MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.6MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.7MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.7MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.7MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.7MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.7MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.7MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.7MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.7MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.7MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.8MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.8MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.8MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.8MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.8MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.8MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.8MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.8MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.8MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.8MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.9MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.9MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.9MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.9MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.9MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.9MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.9MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.9MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.9MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.9MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.0MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.1MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.2MB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.3MB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.3MB 21.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 49.2MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1hzzzfMlec-"
      },
      "source": [
        "pip install pyspark\n",
        "\n",
        "\n",
        "pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUW-7Uu-23le",
        "outputId": "4987e3a4-7ec7-4b87-c87e-2a9c4f711e1b"
      },
      "source": [
        "pip install matplotlib"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0-PkA-zlpgx"
      },
      "source": [
        "#RuntimeError: CUDA error: device-side assert triggered \n",
        "#Loss item RuntimeError CUDA error: device-side assert triggered 나서 레이블 시작을 1에서 0으로 바꿈\n",
        "#train_data['label'] = (train_data['label'] -1 )\n",
        "# the class labels to start at 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKXLb7Ot-ogN",
        "outputId": "64b1aa3b-d7d1-493b-c8fc-32b96744358a"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVfzNcrBYfAc"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from tqdm import tqdm"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "Nf2vtX10omeO",
        "outputId": "38f20cfc-875f-4f1f-d6e3-b6d280d05658"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/yelp_review_polarity_csv/train.csv\",  nrows = 3000, delimiter=',', header=None, names=['label', 'sentence'])\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/yelp_review_polarity_csv/test.csv\",  nrows = 3000, delimiter=',', header=None, names=['label', 'sentence'])\n",
        "\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(train_data.shape[0]))\n",
        "print('Number of test sentences: {:,}\\n'.format(test_data.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "#train_data.head()\n",
        "train_data.head()\n",
        "#df.sample(10)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 3,000\n",
            "\n",
            "Number of test sentences: 3,000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>I'm writing this review to give you a heads up...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>All the food is great here. But the best thing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                           sentence\n",
              "0      1  Unfortunately, the frustration of being Dr. Go...\n",
              "1      2  Been going to Dr. Goldberg for over 10 years. ...\n",
              "2      1  I don't know what Dr. Goldberg was like before...\n",
              "3      1  I'm writing this review to give you a heads up...\n",
              "4      2  All the food is great here. But the best thing..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C3XHkYXRlxb",
        "outputId": "97913a08-4d89-4821-9497-7b6cb2b73f2c"
      },
      "source": [
        "train_data.value_counts()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label  sentence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
              "2      when tram's is out of the equation, this is my backup. just as good.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1\n",
              "1      Pizza ok on it's own nothing great . Definitely not worth the restaurant price.  definitely not like pizzeria UNO in Chicago. Are they even related   or are they just using their name and trademark.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1\n",
              "       Poor service that leaves tenants to fight mildew on their own. When asked to repair something, Rockwel Realty will do it on their terms and refuse to give you notice of when they will enter your apartment despite multiple requests\\n\\nWhen asked to clean mildew, they did in fact clean it with bleach while also spilling bleach on my sheets and ruining them. When told about this problem and that the mildew returned they said it was my fault for not turning the heat up and that damage done during service is an inherent risk the tenant is responsible for and nothing could be done. Their solution was for me to clean the mildew instead of properly insulating the unit to prevent mildew. \\n\\nThe entire building also has severe electrical problems that causes all lights to be replaced twice a month. No permanent fixes are made to existing units, but there is constant construction to expand an old building so they can collect more rent.                             1\n",
              "       Poor service and food. I waited 20 minutes at the bar for someone to just acknowledge my presence. They finally brought my food out and left it without asking for me to pay.... So I had to wait an additional 15 minutes to find someone to pay. Yes, I had to TRY and give them money.\\n\\nOn to the important part... Food was terrible. I had a burger and it was pathetic.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1\n",
              "       Poor excuse for seafood!   The place is overrated.  Good place to have drinks on a Saturday or Sunday afternoon-but that's about it.  The employees are anything but friendly. Unless you sit at the bar, the service is poor. Isn't even that great at the bar.  If you want good seafood, go a couple blocks up the street to Luke Wholeys.  So much better quality!  This place is definitely not for those with sophisticated palates!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ..\n",
              "2      I am a history fan...so when a friend told me about this I made sure he took me to go see it.  The property is very beautiful...\\nWe waited a few minutes and signed in....surprising it was discount day!  The waiting area itself was exquisite and already gave rich history of the property and the people who had lived there. By the time we finished looking it over, our tour guide came in.  She was perfect...very knowledgeable and sweet.  She answered any questions we had and made us feel very welcome there.  She gave us ample time to roam around each room.  \\n\\nThe house is amazing and moreso the car collection.  That is free...and a must-see.  Also is you like art don't miss the art collection.                                                                                                                                                                                                                                                                           1\n",
              "       I am a foodie so when I travel, I must try foods that are famous in the city that I visit. So, Primati Bros is a must on my Pittsburgh trip. Well, I was not disappoint after eating their #2 almost famous pittsburger.  Next time, I will try the pastrami. My friend had the pastrami, and it looked tastier.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
              "       I am a big fan of Max;;s for their local flair, real German food, and authentic Pittsburgh feeling. They did not sell out, are not overly commercialized, and should be supported for the long standing quality service to the city. Thanks, Max's.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1\n",
              "       I always used to avoid Whole Foods because I thought it was way more expensive than Giant Eagle or Trader Joe's.  I recently moved to a new apartment and made a quick run to Whole Foods to stock up on groceries.  It's now the closest grocery store to me, so I figured, why not just this once?\\n\\nThere was tons of interesting looking stuff, and I found myself getting carried away and filling up my cart way more than I had intended.  I braced myself for the total in the checkout line and was pleasantly surprised when it wasn't much more than I typically spend at Giant Eagle.  On top of that, the produce stayed fresh for way longer than GE's typically does!  From now on I'm going to stick with WF for all my fresh produce needs (along with a few other specialty items -- some of their frozen Indian food is surprisingly good), but I'll still go to GE for my everyday grocery shopping.\\n\\nSure, their parking lot is a madhouse, but so is the Market District's.    1\n",
              "1      $400 IN DAMAGES\\n\\nThey cracked my oil pan and it cost me four hundred dollars to repair. Everything was fine before I brought it here. Mechanic said it looked like they put my bolt in crooked and then hammered it in. Avoid at all cost. They will ruin your car.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1\n",
              "Length: 3000, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fesBrQTZNqF",
        "outputId": "2d1e169f-8aa2-4e77-e1fb-c4c09627e6bd"
      },
      "source": [
        "train_data['label'].unique()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywgx7VW7ZdTf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aedd62da-fb13-49e0-8576-b6de12bd149c"
      },
      "source": [
        "train_data['label'] = (train_data['label'] -1 )\n",
        "train_data['label'].unique()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "Xwa4LaoY4dSJ",
        "outputId": "0486f362-8f26-40c5-a405-10bff83cc937"
      },
      "source": [
        "'''\n",
        "# pyspark\n",
        "train_data = train_data.sampleby('label',\n",
        "                                 frac={'1':0.2,\n",
        "                                       '2':0.2,\n",
        "                                       '3':0.2,\n",
        "                                       '4':0.2,\n",
        "                                       '5':0.2},\n",
        "                                 seed=1234)\n",
        "train_data.groupby('label').count().orderby('label').show()\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# pyspark\\ntrain_data = train_data.sampleby('label',\\n                                 frac={'1':0.2,\\n                                       '2':0.2,\\n                                       '3':0.2,\\n                                       '4':0.2,\\n                                       '5':0.2},\\n                                 seed=1234)\\ntrain_data.groupby('label').count().orderby('label').show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OmEQdZOqygD",
        "outputId": "aa32c184-87a1-4f8e-8256-8db3fa8b68ed"
      },
      "source": [
        "sentences = train_data.sentence.values\n",
        "labels = train_data.label.values\n",
        "\n",
        "sentences\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\",\n",
              "       \"Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\",\n",
              "       \"I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\",\n",
              "       ...,\n",
              "       \"I've eaten at Alexander's for a decade, but, like Del's, it's becoming one of those places where the food gets worse every time you go there.  Today, half the lettuce I had on my Caesar salad was rotten.  I have no idea why it ever left the kitchen.  And it was take-out, too, so there wasn't much I could do about it after driving all the way home.  (They gave me the wrong entr\\\\u00e9e too, but at least it was edible.)\\\\n\\\\nI finally think I'm done with this place.\",\n",
              "       \"I have been coming to Alexander's  off and on for almost 25 years. I think in all that time I have only had one meal that was below par. Pretty good average I'd say. I went the other night with my adult daughter. I had the calamari with pasta, a cocktail ( yum, yum), and a garden salad. The salad was very fresh and arrived promptly. The cocktail was strong but great. The calamari arrived quickly and was piled so high on the plate I could barely see over the plate! It was served on what tasted like fresh tomatoes over the pasta. It was so good I couldn't stop eating it, even after I was full. \\\\n\\\\nMy daughter had the salad too with linguini and clam sauce. It was loaded with clams. They were tasty and not rubbery. She suspected they were canned but since they were tender it was okay. The sauce was fragrant too, not fishy. \\\\n\\\\nOur server was super efficient and polite. She kept up with us without being intrusive. The decor of the restaurant has been updated a bit over time. It's cute. A bit too bright for an Italian restaurant if you ask me, but I'm from New England where Italian places are all dimly lit. \\\\n\\\\nOverall, a wonderful, easy going, friendly  neighborhood place to go to relax over some good food and drink. Have a few laughs without a tv interrupting your dinner conversation. You have to sit in the bar for that. Take your mom. Take the family. Take your husband when you can't stand another night of cooking. Just go it alone and who cares about anyone else?! Just have fun eating some really good food that tastes like someone cares.\",\n",
              "       'I AM BOYCOTTING ALEXANDER\\'S FOR LIFE!!! The manager SCREAMED AT ME at the top of his lungs in an epic meltdown that lasted 10 minutes! \\\\n\\\\nOn Sat., April 12, 2014, I attended the 50th birthday party for my dear friend, David Flynn. I organized the party, but relied on Dave and his sister to provide me with the final head count. I made the reservation for 25 people.\\\\n\\\\nThe manager and I played phone tag for the two days before the event. I called back and left messages TWICE to confirm, but apparently he never got my messages. (This was part of his rant, his indignation at unreturned phone calls that actually HAD been returned!)\\\\n\\\\nThe dinner started at 7 PM. Most of our group arrived on time, but some of our group (including me) arrived between 8 PM-9 PM. I was working on a homemade birthday present for Dave, which took longer than expected. I called Dave, his sister and my friends (Reesa & Joel) to tell them I was running late. In hindsight, I should have also called the restaurant to let them know, and for that I am truly sorry. That still doesn\\'t justify the manager SCREAMING AT ME FOR 10 MINUTES!\\\\n\\\\nAfter enjoying a delicious dinner, drinks and birthday cake, none of us had a clue that anything was wrong. If the decision to return to a restaurant were based SOLELY on the food and wait staff, I would definitely return to Alexander\\'s. But sadly, when the manager has a nuclear meltdown and throws a 10-MINUTE SCREAMING TEMPER TANTRUM, it tends to sour one on returning to a restaurant.\\\\n\\\\nTowards the end of our dinner, some of us started to smell cigarette smoke, which we later realized was coming from the patio out front. (It was a warm night, and they had the front sliding-glass doors open.) In the midst of the smoke discussion, the manager approached our table and began an EPIC RANT, SCREAMING AT ME FOR 10 MINUTES!!!\\\\n\\\\nHe was angry that everyone in our group hadn\\'t arrived at exactly 7 PM. The restaurant\\'s kitchen is open till 10 PM on Sat. night, so I don\\'t understand why this was a big deal. We ended up with 20 people total and 5 no-shows. I am always disappointed by no-shows, but I explained this possibility to the manager when I made the reservation. I STILL BROUGHT HIM 20 PAYING CUSTOMERS! \\\\n\\\\nBTW...the dining room was practically EMPTY, which is not what I would expect on a Saturday night in Pittsburgh. Maybe I\\'m not the first customer to be on the receiving end of the manager\\'s meltdown.\\\\n\\\\nThe manager SCREAMED that he had called me twice, and I hadn\\'t called him back! He ranted about this about a dozen times. I told him I HAD called him back - TWICE. It\\'s not my fault if he didn\\'t get my messages. \\\\n\\\\nHe SCREAMED that he had put an extra waitress and cook on staff because he expected 25 people. He SCREAMED that I had cost the restaurant $100! How much money do you think 20 people spent on appetizers, dinner and drinks? I find it hard to believe that they didn\\'t make any money, or at least break even. \\\\n\\\\nI am sorry that there were 5 no-shows in our group, but I always thought Saturday night was the busiest night of the week for most restaurants. I didn\\'t realize the restaurant would look like a deserted, old western ghost town with hardly any customers. On a Saturday night, I assumed they would have a full staff on duty with or without my reservation. I was wrong. I\\'m sorry. I\\'m not perfect.\\\\n\\\\nThe manager also YELLED at me and my friends for \\\\\"LOOKING COMFORTABLE\\\\\" in his restaurant!!! Since when is looking comfortable an OFFENSE??? I thought restaurants wanted you to feel comfortable, but apparently not at Alexander\\'s.\\\\n\\\\nHe also SCREAMED that one of the waitresses only got a $1.00 tip! I found that hard to believe, but just in case he wasn\\'t making it up, I tipped my waitress $35 on my $26 dinner check! I am a former waitress, and I would feel HORRIBLE if any waitress every got ripped off on my watch! Even if his story wasn\\'t true, I figured she deserved an extra big tip just for having to WORK in such a horrible place where the manager YELLS at paying customers!!! If he treats the customers this way, I can only imagine how he treats the employees. \\\\n\\\\nTHE MANAGER\\'S RUDE BEHAVIOR HAS COST HIM MY BUSINESS AND MY FRIENDS\\' BUSINESS FOREVER!!! During his rant, he actually SCREAMED that he never wanted to see me in his restaurant ever again! I\\'m happy to grant him that wish. And so are my friends. \\\\n\\\\nThis is an example of the WORST CUSTOMER SERVICE I have ever witnessed in my entire life!!! I thought the manager was going to have a heart attack or a stroke the way he was SCREAMING FOR 10 MINUTES!\\\\n\\\\nSCREAMING at paying customers is completely unacceptable behavior, and I will NEVER eat at Alexander\\'s ever again. It\\'s really a shame because I loved their salmon with pesto sauce. But great food can\\'t make up for a manager who throws a TEMPER TANTRUM and SCREAMS AT YOU. Also, you can smell smoke in the dining room, which sucks. My friends and I are BOYCOTTING ALEXANDER\\'S FOR LIFE!!!'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xT36S4FrKq-",
        "outputId": "2a2db50f-fc8a-45d8-cf92-e5634ea861c7"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IU-QB5nsPaE",
        "outputId": "abf1cec6-2147-4700-8178-f6afae42213d"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\n",
            "Tokenized:  ['unfortunately', ',', 'the', 'frustration', 'of', 'being', 'dr', '.', 'goldberg', \"'\", 's', 'patient', 'is', 'a', 'repeat', 'of', 'the', 'experience', 'i', \"'\", 've', 'had', 'with', 'so', 'many', 'other', 'doctors', 'in', 'nyc', '-', '-', 'good', 'doctor', ',', 'terrible', 'staff', '.', 'it', 'seems', 'that', 'his', 'staff', 'simply', 'never', 'answers', 'the', 'phone', '.', 'it', 'usually', 'takes', '2', 'hours', 'of', 'repeated', 'calling', 'to', 'get', 'an', 'answer', '.', 'who', 'has', 'time', 'for', 'that', 'or', 'wants', 'to', 'deal', 'with', 'it', '?', 'i', 'have', 'run', 'into', 'this', 'problem', 'with', 'many', 'other', 'doctors', 'and', 'i', 'just', 'don', \"'\", 't', 'get', 'it', '.', 'you', 'have', 'office', 'workers', ',', 'you', 'have', 'patients', 'with', 'medical', 'needs', ',', 'why', 'isn', \"'\", 't', 'anyone', 'answering', 'the', 'phone', '?', 'it', \"'\", 's', 'inc', '##omp', '##re', '##hen', '##sible', 'and', 'not', 'work', 'the', 'ag', '##gra', '##vation', '.', 'it', \"'\", 's', 'with', 'regret', 'that', 'i', 'feel', 'that', 'i', 'have', 'to', 'give', 'dr', '.', 'goldberg', '2', 'stars', '.']\n",
            "Token IDs:  [6854, 1010, 1996, 9135, 1997, 2108, 2852, 1012, 18522, 1005, 1055, 5776, 2003, 1037, 9377, 1997, 1996, 3325, 1045, 1005, 2310, 2018, 2007, 2061, 2116, 2060, 7435, 1999, 16392, 1011, 1011, 2204, 3460, 1010, 6659, 3095, 1012, 2009, 3849, 2008, 2010, 3095, 3432, 2196, 6998, 1996, 3042, 1012, 2009, 2788, 3138, 1016, 2847, 1997, 5567, 4214, 2000, 2131, 2019, 3437, 1012, 2040, 2038, 2051, 2005, 2008, 2030, 4122, 2000, 3066, 2007, 2009, 1029, 1045, 2031, 2448, 2046, 2023, 3291, 2007, 2116, 2060, 7435, 1998, 1045, 2074, 2123, 1005, 1056, 2131, 2009, 1012, 2017, 2031, 2436, 3667, 1010, 2017, 2031, 5022, 2007, 2966, 3791, 1010, 2339, 3475, 1005, 1056, 3087, 10739, 1996, 3042, 1029, 2009, 1005, 1055, 4297, 25377, 2890, 10222, 19307, 1998, 2025, 2147, 1996, 12943, 17643, 21596, 1012, 2009, 1005, 1055, 2007, 9038, 2008, 1045, 2514, 2008, 1045, 2031, 2000, 2507, 2852, 1012, 18522, 1016, 3340, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "2SvMaQl7suza",
        "outputId": "dd0e38db-adae-41da-9efd-c83f0308de11"
      },
      "source": [
        "'''\n",
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "'''"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nmax_len = 0\\n\\n# For every sentence...\\nfor sent in sentences:\\n\\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\\n\\n    # Update the maximum sentence length.\\n    max_len = max(max_len, len(input_ids))\\n\\nprint('Max sentence length: ', max_len)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbMKyVyVtsuV",
        "outputId": "89a63ed1-bce0-427b-f41b-533729d06556"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation=True\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\n",
            "Token IDs: tensor([  101,  6854,  1010,  1996,  9135,  1997,  2108,  2852,  1012, 18522,\n",
            "         1005,  1055,  5776,  2003,  1037,  9377,  1997,  1996,  3325,  1045,\n",
            "         1005,  2310,  2018,  2007,  2061,  2116,  2060,  7435,  1999, 16392,\n",
            "         1011,  1011,  2204,  3460,  1010,  6659,  3095,  1012,  2009,  3849,\n",
            "         2008,  2010,  3095,  3432,  2196,  6998,  1996,  3042,  1012,  2009,\n",
            "         2788,  3138,  1016,  2847,  1997,  5567,  4214,  2000,  2131,  2019,\n",
            "         3437,  1012,  2040,  2038,  2051,  2005,  2008,  2030,  4122,  2000,\n",
            "         3066,  2007,  2009,  1029,  1045,  2031,  2448,  2046,  2023,  3291,\n",
            "         2007,  2116,  2060,  7435,  1998,  1045,  2074,  2123,  1005,  1056,\n",
            "         2131,  2009,  1012,  2017,  2031,  2436,  3667,  1010,  2017,  2031,\n",
            "         5022,  2007,  2966,  3791,  1010,  2339,  3475,  1005,  1056,  3087,\n",
            "        10739,  1996,  3042,  1029,  2009,  1005,  1055,  4297, 25377,  2890,\n",
            "        10222, 19307,  1998,  2025,  2147,  1996, 12943, 17643, 21596,  1012,\n",
            "         2009,  1005,  1055,  2007,  9038,  2008,  1045,  2514,  2008,  1045,\n",
            "         2031,  2000,  2507,  2852,  1012, 18522,  1016,  3340,  1012,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNY-VSFK1koI",
        "outputId": "bba33c51-55a3-4307-9c64-1cd92a004781"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2,700 training samples\n",
            "  300 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqKO6NGY16ut"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWpP5evN6rnL",
        "outputId": "321c30ea-b590-4f23-fc98-246ba9382f26"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 5, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "    attention_probs_dropout_prob = 0.1,\n",
        "    hidden_dropout_prob = 0.1\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-ElBp_NNfaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbd79122-65af-4171-9a78-727a4659c819"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (5, 768)\n",
            "classifier.bias                                                 (5,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjW3hB2_wkjB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a85fd7f5-1622-45ad-cd8f-46e70616bb40"
      },
      "source": [
        " '''\n",
        " # Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters  = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "'''"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Prepare optimizer and schedule (linear warmup and decay)\\nno_decay = ['bias', 'LayerNorm.weight']\\noptimizer_grouped_parameters  = [\\n   {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\\n   {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\\n   ]\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIClet4jOAW3"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  \n",
        "                )"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiQIjC5cOGPm"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsvM5VCvOIDZ"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErJCZN_COUse"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6II6RuCPOYRA"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBdHQ1cHOcnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15082050-960b-4c76-9c76-1bc07cc18490"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        \n",
        "        outputs = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        \n",
        "        loss, logits = outputs['loss'], outputs['logits']\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            outputs = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            loss, logits = outputs['loss'], outputs['logits']\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     85.    Elapsed: 0:00:56.\n",
            "  Batch    80  of     85.    Elapsed: 0:01:51.\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.33\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     85.    Elapsed: 0:00:55.\n",
            "  Batch    80  of     85.    Elapsed: 0:01:51.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:01:57\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.26\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     85.    Elapsed: 0:00:55.\n",
            "  Batch    80  of     85.    Elapsed: 0:01:51.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:01:57\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation Loss: 0.31\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     85.    Elapsed: 0:00:55.\n",
            "  Batch    80  of     85.    Elapsed: 0:01:51.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:57\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation Loss: 0.36\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:08:07 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVe0VsL-XLc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "0ce3bed7-5589-41fd-a91b-1ff0e90e1a60"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:01:58</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.15</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0:01:57</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.04</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0:01:57</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0:01:57</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.45         0.33           0.90       0:01:58         0:00:05\n",
              "2               0.15         0.26           0.92       0:01:57         0:00:05\n",
              "3               0.04         0.31           0.93       0:01:57         0:00:05\n",
              "4               0.01         0.36           0.93       0:01:57         0:00:05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXhfW3hfXYqo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "3b871941-586a-47c2-ff3d-a2301c654d46"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv4GtgQnXppG"
      },
      "source": [
        "##Performance On Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNEf_SOBXxy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d493446b-66a2-4fb8-f5e3-0a082d860282"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/yelp_review_polarity_csv/test.csv\", nrows= 3000, delimiter=',', header=None, names=['label', 'sentence'])\n",
        "test_data['label'] = (test_data['label'] -1 ) # the class labels to start at 0\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(test_data.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = test_data.sentence.values\n",
        "labels = test_data.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 3,000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56FBOoS9YeuP"
      },
      "source": [
        "## Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLL4uaKhc7VH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41ec7bb-9e93-42b4-a855-7343291b03d2"
      },
      "source": [
        "\n",
        "t0 = time.time()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "for step, batch in enumerate(prediction_dataloader):\n",
        "    if step % 40 == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(prediction_dataloader), elapsed))\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    with torch.no_grad():     \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "    \n",
        "    logits = outputs[0]\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "print(\"\")\n",
        "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of     94.    Elapsed: 0:00:19.\n",
            "  Batch    80  of     94.    Elapsed: 0:00:38.\n",
            "\n",
            "Accuracy: 0.94\n",
            "Test took: 0:00:45\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
